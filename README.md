# Spark - Map-Reduce Querying and Optimization

## About the Project
This is the project for the M111 Big Data Management Course in DSIT, Master's (Data Science and Artificial Intelligence).The task of the project
is to implement joins using different algorithms and API (Rdd / Dataframe) , time and compare their executions and optimize the joins using faster algorithms 
and implementations.The task is split into different subtasks and the results are all located in different subfolders.

## Built with 
```
Python
Spark
PySpark
Hadoop
```
You can find out more about the installation of Spark here :
(https://spark.apache.org/)
(https://spark.apache.org/downloads.html)
and how to set up a single Hadoop cluster here :
(https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)


## Usage
The project is implemented using HADOOP DISTRIBUTED FILESYSTEM (HDFS) and SPARK.There are various guides online that show how to get a working 
HDFS cluster and how to install Spark so the installation is outside of the scope of the project. After having installed HADOOP and SPARK you can proceed to the queries asked.
Each task is described in it's corresponding unit in the pdf. 
1. To get started take a look into the `Report.pdf` where all tasks and subtasks results are described and explained fully.
2. Next you can move to the  `src ` directory, where you will find the code for all the tasks split into subdirectories.
3. The  `output` directory contains the output files-results that each part of the code produced.
